{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_oA7HEc04Qv",
        "outputId": "c20cfe62-d9b0-4e9d-9c79-a2f9fdd96d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=3e9958ba21b3416b144287c9473b2ff8c3c604bb4e60478a4e5ee1b9f1e8f5d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import nltk\n",
        "import spacy\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "Zht3SuQqyBao",
        "outputId": "23f52a2b-55d4-41a1-e8b5-c8449fe25d1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  country sellerId                                    sellerName  \\\n",
              "0      PE  SCD0DE7                                  MODA & COLOR   \n",
              "1      PE  SCA3D8E                                    DS VENTURE   \n",
              "2      PE  SC66A00                                     WELL DONE   \n",
              "3      PE  SC63E40  IMPORTACIONES ELODI SAC  IMPORTACIONES ELODI   \n",
              "4      PE  SC89B3D                                ANDES PERU UAT   \n",
              "\n",
              "                                sku_seller  \\\n",
              "0                         FBFL00508-100-00   \n",
              "1                              SE0649-20-3   \n",
              "2                                   120015   \n",
              "3          348_DELETED_2023-07-16_19-32-48   \n",
              "4  20221013230_DELETED_2022-11-17_18-16-28   \n",
              "\n",
              "                                        product_name brand_name  \\\n",
              "0  Suplemento Colageno Hidrolizado Con Camu Camu ...      ZOHAR   \n",
              "1                 Vibrador Rabbit - Echanter Exciter   FOREPLAY   \n",
              "2          NUTRAZUL ARNICA SPORT SPRAY FRASCO X 75ML  PHARMARIS   \n",
              "3  100 Guantes Sinteticos Desechables de Nitrilo ...   GENERICO   \n",
              "4                       A - Test product bulk test 3      LIMAR   \n",
              "\n",
              "                            model   shop_sku  \\\n",
              "0            COLAGENO HIDROLIZADO  124738632   \n",
              "1                        Foreplay  125099838   \n",
              "2                           spray  125569598   \n",
              "3  Guantes de color negro Talla M  121811312   \n",
              "4                        TEST 003  115223874   \n",
              "\n",
              "                       primary_category global_identifier  ...  \\\n",
              "0            Medicamentos farmaceuticos             G1606  ...   \n",
              "1            Medicamentos farmaceuticos             G1606  ...   \n",
              "2            Medicamentos farmaceuticos             G1606  ...   \n",
              "3              Mobiliario de peluqueria             G1608  ...   \n",
              "4  Armarios|gabinetes para herramientas           G010101  ...   \n",
              "\n",
              "  rlo_items_problema_cobro rlo_items_producto_incompleto  \\\n",
              "0                     <NA>                          <NA>   \n",
              "1                     <NA>                          <NA>   \n",
              "2                     <NA>                          <NA>   \n",
              "3                     <NA>                          <NA>   \n",
              "4                     <NA>                          <NA>   \n",
              "\n",
              "  rlo_items_publicidad_enganosa rlo_items_falla_producto customer_reviews  \\\n",
              "0                          <NA>                     <NA>             <NA>   \n",
              "1                          <NA>                     <NA>             <NA>   \n",
              "2                          <NA>                     <NA>             <NA>   \n",
              "3                          <NA>                     <NA>             <NA>   \n",
              "4                          <NA>                     <NA>             <NA>   \n",
              "\n",
              "  customer_reviews_1 customer_reviews_2 customer_reviews_3 customer_reviews_4  \\\n",
              "0               <NA>               <NA>               <NA>               <NA>   \n",
              "1               <NA>               <NA>               <NA>               <NA>   \n",
              "2               <NA>               <NA>               <NA>               <NA>   \n",
              "3               <NA>               <NA>               <NA>               <NA>   \n",
              "4               <NA>               <NA>               <NA>               <NA>   \n",
              "\n",
              "  customer_reviews_5  \n",
              "0               <NA>  \n",
              "1               <NA>  \n",
              "2               <NA>  \n",
              "3               <NA>  \n",
              "4               <NA>  \n",
              "\n",
              "[5 rows x 176 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72ca6a9c-4ab0-46b8-8366-145547fe6ddf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>sellerId</th>\n",
              "      <th>sellerName</th>\n",
              "      <th>sku_seller</th>\n",
              "      <th>product_name</th>\n",
              "      <th>brand_name</th>\n",
              "      <th>model</th>\n",
              "      <th>shop_sku</th>\n",
              "      <th>primary_category</th>\n",
              "      <th>global_identifier</th>\n",
              "      <th>...</th>\n",
              "      <th>rlo_items_problema_cobro</th>\n",
              "      <th>rlo_items_producto_incompleto</th>\n",
              "      <th>rlo_items_publicidad_enganosa</th>\n",
              "      <th>rlo_items_falla_producto</th>\n",
              "      <th>customer_reviews</th>\n",
              "      <th>customer_reviews_1</th>\n",
              "      <th>customer_reviews_2</th>\n",
              "      <th>customer_reviews_3</th>\n",
              "      <th>customer_reviews_4</th>\n",
              "      <th>customer_reviews_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PE</td>\n",
              "      <td>SCD0DE7</td>\n",
              "      <td>MODA &amp; COLOR</td>\n",
              "      <td>FBFL00508-100-00</td>\n",
              "      <td>Suplemento Colageno Hidrolizado Con Camu Camu ...</td>\n",
              "      <td>ZOHAR</td>\n",
              "      <td>COLAGENO HIDROLIZADO</td>\n",
              "      <td>124738632</td>\n",
              "      <td>Medicamentos farmaceuticos</td>\n",
              "      <td>G1606</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PE</td>\n",
              "      <td>SCA3D8E</td>\n",
              "      <td>DS VENTURE</td>\n",
              "      <td>SE0649-20-3</td>\n",
              "      <td>Vibrador Rabbit - Echanter Exciter</td>\n",
              "      <td>FOREPLAY</td>\n",
              "      <td>Foreplay</td>\n",
              "      <td>125099838</td>\n",
              "      <td>Medicamentos farmaceuticos</td>\n",
              "      <td>G1606</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PE</td>\n",
              "      <td>SC66A00</td>\n",
              "      <td>WELL DONE</td>\n",
              "      <td>120015</td>\n",
              "      <td>NUTRAZUL ARNICA SPORT SPRAY FRASCO X 75ML</td>\n",
              "      <td>PHARMARIS</td>\n",
              "      <td>spray</td>\n",
              "      <td>125569598</td>\n",
              "      <td>Medicamentos farmaceuticos</td>\n",
              "      <td>G1606</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PE</td>\n",
              "      <td>SC63E40</td>\n",
              "      <td>IMPORTACIONES ELODI SAC  IMPORTACIONES ELODI</td>\n",
              "      <td>348_DELETED_2023-07-16_19-32-48</td>\n",
              "      <td>100 Guantes Sinteticos Desechables de Nitrilo ...</td>\n",
              "      <td>GENERICO</td>\n",
              "      <td>Guantes de color negro Talla M</td>\n",
              "      <td>121811312</td>\n",
              "      <td>Mobiliario de peluqueria</td>\n",
              "      <td>G1608</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PE</td>\n",
              "      <td>SC89B3D</td>\n",
              "      <td>ANDES PERU UAT</td>\n",
              "      <td>20221013230_DELETED_2022-11-17_18-16-28</td>\n",
              "      <td>A - Test product bulk test 3</td>\n",
              "      <td>LIMAR</td>\n",
              "      <td>TEST 003</td>\n",
              "      <td>115223874</td>\n",
              "      <td>Armarios|gabinetes para herramientas</td>\n",
              "      <td>G010101</td>\n",
              "      <td>...</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72ca6a9c-4ab0-46b8-8366-145547fe6ddf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-72ca6a9c-4ab0-46b8-8366-145547fe6ddf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-72ca6a9c-4ab0-46b8-8366-145547fe6ddf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1c23b036-7426-47db-b0cf-30a51c53de69\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1c23b036-7426-47db-b0cf-30a51c53de69')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1c23b036-7426-47db-b0cf-30a51c53de69 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Autenticación\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Proyecto y cliente de BigQuery\n",
        "project_id = 'bi-fcom-drmb-local-pe-sbx'\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Consulta\n",
        "query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM `bi-fcom-drmb-local-pe-sbx.Dragonite_SX_KPIs.reporte_productos`\n",
        "    WHERE country = 'PE'\n",
        "\"\"\"\n",
        "\n",
        "# Cargar el DataFrame\n",
        "df_spm = client.query(query).to_dataframe()\n",
        "\n",
        "# Ver las primeras filas del DataFrame\n",
        "df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_spm.shape)"
      ],
      "metadata": {
        "id": "4zjz61NjBySI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83864bf-e547-4cba-bfa0-7c157615f5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1348283, 176)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLkcZJlF3F0R"
      },
      "outputs": [],
      "source": [
        "# Cargar el conjunto de datos desde el DataFrame de BigQuery\n",
        "df = df_spm\n",
        "\n",
        "# Lista de atributos que deseas mantener\n",
        "atributos_deseados = [\n",
        "    'country', 'sellerId', 'sellerName', 'sku_seller', 'product_name','shop_sku',\n",
        "    'primary_category','N1', 'size', 'width_in_cm','length_in_cm', 'height_in_cm',\n",
        "    'weight_in_kg', 'width_diff', 'length_diff','height_diff', 'weight_diff'\n",
        "]\n",
        "\n",
        "# Crear un nuevo DataFrame solo con los atributos deseados y renombrarlo a df_spm\n",
        "df_spm = df[atributos_deseados]\n",
        "\n",
        "# Mostrar las primeras filas del nuevo DataFrame\n",
        "#df_spm.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtH1RcrL3TKj"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Ver cantidad de nulos\n",
        "#df_spm.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmsFh5nK3X7p"
      },
      "outputs": [],
      "source": [
        "# Eliminar todas las filas que tengan nulos\n",
        "df_spm = df_spm.dropna()\n",
        "\n",
        "#Ver la cantidad de nulos, post imputación\n",
        "#df_spm.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW1QYOAc3kXO"
      },
      "outputs": [],
      "source": [
        "# Suma de los valores \"No\" y \"Sí\" para cada columna\n",
        "suma_no_si_width_diff = df_spm['width_diff'].value_counts().to_dict()\n",
        "suma_no_si_length_diff = df_spm['length_diff'].value_counts().to_dict()\n",
        "suma_no_si_height_diff = df_spm['height_diff'].value_counts().to_dict()\n",
        "suma_no_si_weight_diff = df_spm['weight_diff'].value_counts().to_dict()\n",
        "\n",
        "# Mostrar los resultados\n",
        "#print(\"Suma de 'No' y 'Sí' en width_diff:\", suma_no_si_width_diff)\n",
        "#print(\"Suma de 'No' y 'Sí' en length_diff:\", suma_no_si_length_diff)\n",
        "#print(\"Suma de 'No' y 'Sí' en height_diff:\", suma_no_si_height_diff)\n",
        "#print(\"Suma de 'No' y 'Sí' en weight_diff:\", suma_no_si_weight_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S976O0QI3p9B"
      },
      "outputs": [],
      "source": [
        "df_spm['target'] = 'Correcto'  # Inicializar con 'Correcto'\n",
        "\n",
        "# Actualizar a 'Incorrecto' si hay algún 'Sí' en las columnas mencionadas\n",
        "condicion_si = (df_spm['width_diff'] == 'Si') | \\\n",
        "               (df_spm['length_diff'] == 'Si') | \\\n",
        "               (df_spm['height_diff'] == 'Si') | \\\n",
        "               (df_spm['weight_diff'] == 'Si')\n",
        "\n",
        "df_spm.loc[condicion_si, 'target'] = 'Incorrecto'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBgZ04og33L8"
      },
      "outputs": [],
      "source": [
        "# Eliminar Incorrectos\n",
        "df_spm_correctos = df_spm[df_spm['target'] == 'Correcto']\n",
        "\n",
        "# Renombrar el DataFrame\n",
        "df_spm = df_spm_correctos\n",
        "\n",
        "# Ver las primeras filas del DataFrame resultante\n",
        "#df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suma de los valores en la columna 'target'\n",
        "suma_target = df_spm['target'].value_counts()\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(suma_target)\n"
      ],
      "metadata": {
        "id": "pyghLZ2XB1Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa50fc1-9df0-4db8-a071-9f33019fe177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correcto    1156729\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjb1uwri4I16"
      },
      "outputs": [],
      "source": [
        "# Lista de atributos que deseas mantener\n",
        "atributos_deseados = [\n",
        "    'sellerName','product_name','shop_sku','primary_category','N1', 'size',\n",
        "    'width_in_cm','length_in_cm', 'height_in_cm','weight_in_kg',\n",
        "]\n",
        "\n",
        "# Crear un nuevo DataFrame solo con los atributos deseados y renombrarlo a df_spm\n",
        "df_spm = df[atributos_deseados]\n",
        "\n",
        "# Mostrar las primeras filas del nuevo DataFrame\n",
        "#df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbijv76C4gUr"
      },
      "outputs": [],
      "source": [
        "df_spm = df_spm.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbedQTG64TQX"
      },
      "outputs": [],
      "source": [
        "#Ver la cantidad de nulos, post imputación\n",
        "#df_spm.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0Zk_Dyk4jux"
      },
      "outputs": [],
      "source": [
        "# Convertir las columnas a tipo numérico\n",
        "df_spm['width_in_cm'] = pd.to_numeric(df_spm['width_in_cm'], errors='coerce')\n",
        "df_spm['length_in_cm'] = pd.to_numeric(df_spm['length_in_cm'], errors='coerce')\n",
        "df_spm['height_in_cm'] = pd.to_numeric(df_spm['height_in_cm'], errors='coerce')\n",
        "\n",
        "# Calcular la columna de volumen en cm3\n",
        "df_spm['volumen_cm3'] = df_spm['width_in_cm'] * df_spm['length_in_cm'] * df_spm['height_in_cm']\n",
        "\n",
        "# Reordenar las columnas colocando 'size' al final\n",
        "column_order = [col for col in df_spm.columns if col != 'size'] + ['size']\n",
        "df_spm = df_spm[column_order]\n",
        "\n",
        "# Verificar las primeras filas del DataFrame actualizado\n",
        "#df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_spm.shape)"
      ],
      "metadata": {
        "id": "wYi_mwvNB3P1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e9125e-4a89-4107-ce61-1e6b2acc916a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1262967, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7gWVwejFgTl"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, CountVectorizer, IDF\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYaIxsxhFntp"
      },
      "outputs": [],
      "source": [
        "# Convertir las columnas a tipo numérico con 4 decimales\n",
        "numeric_columns = ['width_in_cm', 'length_in_cm', 'height_in_cm', 'weight_in_kg', 'volumen_cm3']\n",
        "for col in numeric_columns:\n",
        "    df_spm[col] = pd.to_numeric(df_spm[col], errors='coerce').round(4)\n",
        "\n",
        "# Reordenar las columnas colocando 'size' al final\n",
        "column_order = [col for col in df_spm.columns if col != 'size'] + ['size']\n",
        "df_spm = df_spm[column_order]\n",
        "\n",
        "# Verificar las primeras filas del DataFrame actualizado\n",
        "#df_spm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSTKbxHT8ATY"
      },
      "outputs": [],
      "source": [
        "# Crear un esquema para el DataFrame de Spark\n",
        "schema = StructType([\n",
        "    StructField(\"sellerName\", StringType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"shop_sku\", StringType(), True),\n",
        "    StructField(\"primary_category\", StringType(), True),\n",
        "    StructField(\"N1\", StringType(), True),\n",
        "    StructField(\"width_in_cm\", FloatType(), True),\n",
        "    StructField(\"length_in_cm\", FloatType(), True),\n",
        "    StructField(\"height_in_cm\", FloatType(), True),\n",
        "    StructField(\"weight_in_kg\", FloatType(), True),\n",
        "    StructField(\"volumen_cm3\", FloatType(), True),\n",
        "    StructField(\"size\", StringType(), True),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpTtwjrCFta4"
      },
      "outputs": [],
      "source": [
        "# Inicializar la sesión de Spark\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SPM-SPARK\").getOrCreate()\n",
        "\n",
        "# Crear el DataFrame de Spark a partir del DataFrame de pandas\n",
        "df_spark = spark.createDataFrame(df_spm, schema=schema)\n",
        "\n",
        "# Verificar las primeras filas del DataFrame de Spark\n",
        "#df_spark.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzwom-dr-oFD",
        "outputId": "3a3344e7-e4f3-4687-9a32-637a0a0a2148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.4)\n",
            "2024-01-28 15:49:37.953582: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-28 15:49:37.953623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-28 15:49:37.954883: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-28 15:49:39.042895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.4)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wemHq_XrGXSm",
        "outputId": "ba36caf2-9d7a-4746-a767-c835ad5df90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark.sql.functions as F\n",
        "import nltk\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYwJaYkkGcAT"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo de procesamiento de lenguaje natural en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Obtener las stopwords en español\n",
        "spanish_stop_words = set(nltk.corpus.stopwords.words('spanish'))\n",
        "\n",
        "# Definir la función UDF para lematizar texto\n",
        "def lemmatize_text_udf(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "# Definir las funciones UDF para tokenizar y limpiar texto\n",
        "def tokenize_and_clean_udf(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in spanish_stop_words]\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9eAkMrPHjGc"
      },
      "outputs": [],
      "source": [
        "# Definir las funciones UDF para Spark\n",
        "tokenize_and_clean_spark_udf = udf(tokenize_and_clean_udf, ArrayType(StringType()))\n",
        "lemmatize_text_spark_udf = udf(lemmatize_text_udf, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bil7XUNSIBv3"
      },
      "outputs": [],
      "source": [
        "# Aplicar tokenización y limpieza en un solo paso\n",
        "df_spark = df_spark.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "df_spark = df_spark.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "df_spark = df_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'product_name_tokens')))\n",
        "df_spark = df_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'primary_category_tokens')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elb0sjl0IBg_"
      },
      "outputs": [],
      "source": [
        "# Aplicar lematización\n",
        "df_spark = df_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf('product_name_cleaned'))\n",
        "df_spark = df_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf('primary_category_cleaned'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83KOmBfCIBeU"
      },
      "outputs": [],
      "source": [
        "# TF-IDF para 'product_name' y 'primary_category' en una única pipeline\n",
        "vectorizer_name = CountVectorizer(inputCol='product_name_tokens', outputCol='raw_name_counts_name_tfidf')\n",
        "idf_name = IDF(inputCol='raw_name_counts_name_tfidf', outputCol='name_tfidf')\n",
        "\n",
        "vectorizer_category = CountVectorizer(inputCol='primary_category_tokens', outputCol='raw_category_counts_category_tfidf')\n",
        "idf_category = IDF(inputCol='raw_category_counts_category_tfidf', outputCol='category_tfidf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGjuR-ECIBbT"
      },
      "outputs": [],
      "source": [
        "text_pipeline = Pipeline(stages=[\n",
        "    vectorizer_name, idf_name,\n",
        "    vectorizer_category, idf_category\n",
        "])\n",
        "\n",
        "# Verificar las primeras filas del DataFrame de Spark después del procesamiento de texto\n",
        "#df_spark.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0et22OdGCeTu"
      },
      "outputs": [],
      "source": [
        "#df_spark.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-ffj4mVFpTS"
      },
      "outputs": [],
      "source": [
        "#rdd_from_df_spark = df_spark.rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSSvx0S3DGrr"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Aplicar el pipeline de procesamiento de texto al DataFrame\n",
        "df_spark = text_pipeline.fit(df_spark).transform(df_spark)\n",
        "\n",
        "# Reducción de palabras a las dos primeras en las columnas 'product_name_cleaned' y 'primary_category_cleaned'\n",
        "df_spark = df_spark.withColumn('product_name_cleaned', F.expr(\"substring_index(product_name_cleaned, ' ', 2)\"))\n",
        "df_spark = df_spark.withColumn('primary_category_cleaned', F.expr(\"substring_index(primary_category_cleaned, ' ', 2)\"))\n",
        "\n",
        "# Verificar las primeras filas del DataFrame de Spark después del procesamiento de texto\n",
        "#df_spark.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcKLwYg7Y1Vp"
      },
      "outputs": [],
      "source": [
        "# Crear un StringIndexer para convertir 'size' a etiquetas numéricas\n",
        "indexer = StringIndexer(inputCol='size', outputCol='size_indexed')\n",
        "\n",
        "# Ajustar las columnas de características según tu conjunto de datos, incluyendo las nuevas características de TF-IDF\n",
        "feature_columns = ['width_in_cm', 'length_in_cm', 'height_in_cm', 'weight_in_kg', 'volumen_cm3']\n",
        "\n",
        "# Crear un VectorAssembler para combinar las características en un vector\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "train_data, test_data = df_spark.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Modelo de clasificación\n",
        "classifier = DecisionTreeClassifier(\n",
        "    labelCol='size_indexed',  # Usar la columna size_indexed como etiqueta\n",
        "    featuresCol='features',\n",
        "    maxDepth=28,\n",
        "    minInstancesPerNode=1\n",
        ")\n",
        "\n",
        "# Crear el Pipeline con todas las etapas\n",
        "pipeline = Pipeline(stages=[\n",
        "    indexer,\n",
        "    vector_assembler,\n",
        "    classifier\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2B6fxZfCT6U"
      },
      "outputs": [],
      "source": [
        "# Ajustar el modelo al conjunto de entrenamiento\n",
        "model = pipeline.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones sobre los datos de prueba.\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Visualizar predicciones\n",
        "predictions.select('size', 'size_indexed', 'prediction', 'probability').show()\n",
        "\n",
        "# Crear un evaluador\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='size_indexed', predictionCol='prediction', metricName='accuracy')\n",
        "\n",
        "# Calcular el accuracy\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f'Exactitud del modelo: {accuracy}')\n"
      ],
      "metadata": {
        "id": "6VgdO2-GBqrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO09-Yy7CirP"
      },
      "outputs": [],
      "source": [
        "# Mostrar los valores distintos de size, label y prediction\n",
        "predictions.select('size', 'size_indexed', 'prediction').distinct().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJaP_wrILPpA"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol='size_indexed', predictionCol='prediction', metricName='weightedRecall')\n",
        "recall = evaluator.evaluate(predictions)\n",
        "print(f'Recall del modelo: {recall}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DPa2e_nLqWY"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol='size_indexed', predictionCol='prediction', metricName='weightedPrecision')\n",
        "precision = evaluator.evaluate(predictions)\n",
        "print(f'Precisión del modelo: {precision}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgmFlSy-LtyV"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol='size_indexed', predictionCol='prediction', metricName='f1')\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "print(f'F1-Score del modelo: {f1_score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AVp29Eu_mM7"
      },
      "outputs": [],
      "source": [
        "# Ruta local donde se guardará el modelo\n",
        "local_model_path = \"/content/SPM_spark\"\n",
        "\n",
        "# Guardar el modelo en el sistema de archivos local de Colab\n",
        "model.save(local_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KSTUGkY_xHe"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Comprimir el modelo para descargarlo como un archivo zip\n",
        "!zip -r /content/SPM_spark.zip /content/SPM_spark\n",
        "\n",
        "# Descargar el archivo zip\n",
        "files.download(\"/content/SPM_spark.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_boroU68CSmj"
      },
      "source": [
        "# --- FINAL DEL MODELO TALLAS ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbo2oYhDCYyJ"
      },
      "source": [
        "# CARGAR EL MODELO Y PROBAR CON UN REGISTRO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrUHsEHKEdtT",
        "outputId": "7b76e9db-0db0-4160-a9e6-d21a79679016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=eb28f6cca8bc74fb5e6f2c8bedb01900a1843f438aeb0854d5119328da1696dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFhDcNk_2YUq"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from pyspark.ml import PipelineModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI32Pv8e5ho8",
        "outputId": "50427ac0-1e5b-47a7-973a-ea2ef1decfc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contenido del directorio:\n",
            "stages\n",
            "metadata\n",
            "\n",
            "¡La carpeta 'stages' existe! Puede contener el modelo.\n",
            "\n",
            "¡La carpeta 'metadata' existe! Puede contener información adicional sobre el modelo.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "model_directory = \"/content/SPM_spark\"\n",
        "\n",
        "# Verificar si el directorio existe\n",
        "if os.path.exists(model_directory):\n",
        "    # Listar el contenido del directorio\n",
        "    content_list = os.listdir(model_directory)\n",
        "\n",
        "    # Imprimir el contenido del directorio\n",
        "    print(\"Contenido del directorio:\")\n",
        "    for item in content_list:\n",
        "        print(item)\n",
        "\n",
        "    # Verificar si hay una carpeta 'stages'\n",
        "    if 'stages' in content_list:\n",
        "        print(\"\\n¡La carpeta 'stages' existe! Puede contener el modelo.\")\n",
        "\n",
        "    # Verificar si hay una carpeta 'metadata'\n",
        "    if 'metadata' in content_list:\n",
        "        print(\"\\n¡La carpeta 'metadata' existe! Puede contener información adicional sobre el modelo.\")\n",
        "else:\n",
        "    print(f\"El directorio {model_directory} no existe.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h59ZoFmN4ZCF"
      },
      "outputs": [],
      "source": [
        "#Extraerel zip\n",
        "# Nombre del archivo zip que subiste\n",
        "zip_filename = 'SPM_spark.zip'\n",
        "\n",
        "# Descomprimir el archivo\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('SPM_spark')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVuljDZSCd2r"
      },
      "outputs": [],
      "source": [
        "#Cargar el modelo\n",
        "# Cargar el modelo desde el directorio descomprimido\n",
        "loaded_model = PipelineModel.load(\"/content/SPM_spark\")\n",
        "\n",
        "# Ahora puedes utilizar el modelo cargado en Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8vwj9fTDkfG"
      },
      "outputs": [],
      "source": [
        "nuevo_registro = {\n",
        "    'sellerName': 'FARMATODO Shop',\n",
        "    'product_name': 'Dolex activgel 24 tabs',\n",
        "    'shop_sku': '119878204',\n",
        "    'primary_category': 'Medicamentos farmaceuticos',\n",
        "    'N1': 'Belleza, cuidado personal, higiene y salud',\n",
        "    'width_in_cm': 10.0,\n",
        "    'length_in_cm': 7.0,\n",
        "    'height_in_cm': 10.0,\n",
        "    'weight_in_kg': 0.3,\n",
        "    'volumen_cm3': 700.0,\n",
        "    'size': 'XS3'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQc_vGr-OwaD"
      },
      "outputs": [],
      "source": [
        "# Importar pandas\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Crear un esquema para el DataFrame de Spark\n",
        "schema = StructType([\n",
        "    StructField(\"sellerName\", StringType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"shop_sku\", StringType(), True),\n",
        "    StructField(\"primary_category\", StringType(), True),\n",
        "    StructField(\"N1\", StringType(), True),\n",
        "    StructField(\"width_in_cm\", FloatType(), True),\n",
        "    StructField(\"length_in_cm\", FloatType(), True),\n",
        "    StructField(\"height_in_cm\", FloatType(), True),\n",
        "    StructField(\"weight_in_kg\", FloatType(), True),\n",
        "    StructField(\"volumen_cm3\", FloatType(), True),\n",
        "    StructField(\"size\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Inicializar la sesión de Spark\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SPM-SPARK\").getOrCreate()\n",
        "\n",
        "\n",
        "# Definir la función UDF para lematizar texto\n",
        "def lemmatize_text_udf(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "# Definir las funciones UDF para tokenizar y limpiar texto\n",
        "def tokenize_and_clean_udf(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in spanish_stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Definir las funciones UDF para Spark\n",
        "tokenize_and_clean_spark_udf = udf(tokenize_and_clean_udf, ArrayType(StringType()))\n",
        "lemmatize_text_spark_udf = udf(lemmatize_text_udf, StringType())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXhOsTPSKtux",
        "outputId": "6a055fdf-b426-4779-f9c4-25279ed29ab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----+------------+----------+--------------------+\n",
            "|        product_name|size|size_indexed|prediction|         probability|\n",
            "+--------------------+----+------------+----------+--------------------+\n",
            "|Dolex activgel 24...| XS3|         1.0|       1.0|[0.0,1.0,0.0,0.0,...|\n",
            "+--------------------+----+------------+----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crear un DataFrame de pandas con el nuevo registro\n",
        "nuevo_registro_pd = pd.DataFrame([nuevo_registro])\n",
        "\n",
        "# Crear un DataFrame de Spark a partir del DataFrame de pandas\n",
        "nuevo_registro_spark = spark.createDataFrame(nuevo_registro_pd, schema=schema)\n",
        "\n",
        "# Aplicar tokenización y limpieza en un solo paso al nuevo registro\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'product_name_tokens')))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'primary_category_tokens')))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf('product_name_cleaned'))\n",
        "nuevo_registro_spark = nuevo_registro_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf('primary_category_cleaned'))\n",
        "\n",
        "# Aplicar el mismo pipeline del modelo al nuevo registro\n",
        "resultado_prediccion = loaded_model.transform(nuevo_registro_spark)\n",
        "\n",
        "# Seleccionar las columnas relevantes del resultado de la predicción\n",
        "resultado_seleccionado = resultado_prediccion.select('product_name', 'size', 'size_indexed', 'prediction', 'probability').show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxcUgPVCJTfo"
      },
      "source": [
        "#TERMINA TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kR06pnLJYEe"
      },
      "source": [
        "# FIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z0JN91CNeV8",
        "outputId": "98bf3c83-0de3-4b52-fa78-9ca76f4c9ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------------+--------+--------------------+------------+-----------+------------+------------+------------+-----------+----+\n",
            "|  sellerName|  product_name|shop_sku|    primary_category|          N1|width_in_cm|length_in_cm|height_in_cm|weight_in_kg|volumen_cm3|size|\n",
            "+------------+--------------+--------+--------------------+------------+-----------+------------+------------+------------+-----------+----+\n",
            "|Sneyder Shop|Refrigueradora|  SKU183|Electronomesticos...|Linea Blanca|     1000.0|      2400.0|       100.0|       300.0|      2.4E8|  EO|\n",
            "+------------+--------------+--------+--------------------+------------+-----------+------------+------------+------------+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "\n",
        "\n",
        "# Crear un esquema para el DataFrame de Spark\n",
        "schema = StructType([\n",
        "    StructField(\"sellerName\", StringType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"shop_sku\", StringType(), True),\n",
        "    StructField(\"primary_category\", StringType(), True),\n",
        "    StructField(\"N1\", StringType(), True),\n",
        "    StructField(\"width_in_cm\", FloatType(), True),\n",
        "    StructField(\"length_in_cm\", FloatType(), True),\n",
        "    StructField(\"height_in_cm\", FloatType(), True),\n",
        "    StructField(\"weight_in_kg\", FloatType(), True),\n",
        "    StructField(\"volumen_cm3\", FloatType(), True),\n",
        "    StructField(\"size\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Inicializar la sesión de Spark\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"SPM-SPARK\").getOrCreate()\n",
        "\n",
        "# Crear el DataFrame de Spark a partir de los nuevos datos\n",
        "nuevos_datos_spark = spark.createDataFrame([nuevo_registro], schema=schema)\n",
        "\n",
        "# Verificar las primeras filas del DataFrame de Spark con los nuevos datos\n",
        "nuevos_datos_spark.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg6mqj0YRCOF",
        "outputId": "8566d8b5-9a37-4a9e-805e-b9a079601e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2024-01-16 03:15:26.388724: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 03:15:26.388790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 03:15:26.390736: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 03:15:28.090898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXyrlqwfQwXq",
        "outputId": "6fb277de-bac4-4da6-9393-41eadecd1512"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark.sql.functions as F\n",
        "import nltk\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Cargar modelo de procesamiento de lenguaje natural en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Obtener las stopwords en español\n",
        "spanish_stop_words = set(nltk.corpus.stopwords.words('spanish'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "aC9gejNbbYjn",
        "outputId": "662947a0-9366-4fe5-b651-c87dc34a4cf2"
      },
      "outputs": [
        {
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-41-39030096985e>\", line 9, in lemmatize_text\n  File \"/usr/local/lib/python3.10/dist-packages/spacy/language.py\", line 1030, in __call__\n    doc = self._ensure_doc(text)\n  File \"/usr/local/lib/python3.10/dist-packages/spacy/language.py\", line 1124, in _ensure_doc\n    raise ValueError(Errors.E1041.format(type=type(doc_like)))\nValueError: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-493349b1bf7d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Verificar las primeras filas del DataFrame de Spark con los nuevos datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnuevos_datos_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Imprimir el esquema del DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnuevos_datos_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-41-39030096985e>\", line 9, in lemmatize_text\n  File \"/usr/local/lib/python3.10/dist-packages/spacy/language.py\", line 1030, in __call__\n    doc = self._ensure_doc(text)\n  File \"/usr/local/lib/python3.10/dist-packages/spacy/language.py\", line 1124, in _ensure_doc\n    raise ValueError(Errors.E1041.format(type=type(doc_like)))\nValueError: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "# Verificar las primeras filas del DataFrame de Spark con los nuevos datos\n",
        "nuevos_datos_spark.show()\n",
        "\n",
        "# Imprimir el esquema del DataFrame\n",
        "nuevos_datos_spark.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "A0mgoAgqRsuB",
        "outputId": "89127139-1522-4b87-cef0-9957c0ac2e05"
      },
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "features does not exist. Available: sellerName, product_name, shop_sku, primary_category, N1, width_in_cm, length_in_cm, height_in_cm, weight_in_kg, volumen_cm3, size, product_name_tokens, primary_category_tokens, product_name_cleaned, primary_category_cleaned",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-39030096985e>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# TF-IDF en 'product_name' y 'primary_category'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Asumiendo que 'loaded_model' y 'model_category' contienen los modelos que necesitas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mnuevos_datos_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuevos_datos_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mnuevos_datos_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_category\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuevos_datos_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: sellerName, product_name, shop_sku, primary_category, N1, width_in_cm, length_in_cm, height_in_cm, weight_in_kg, volumen_cm3, size, product_name_tokens, primary_category_tokens, product_name_cleaned, primary_category_cleaned"
          ]
        }
      ],
      "source": [
        "# Definir funciones de tokenización y limpieza\n",
        "def tokenize_and_clean(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in spanish_stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Definir función de lematización\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "# Crear UDFs de Spark\n",
        "tokenize_and_clean_spark_udf = udf(tokenize_and_clean_udf, ArrayType(StringType()))\n",
        "lemmatize_text_spark_udf = udf(lemmatize_text_udf, StringType())\n",
        "\n",
        "# Aplicar las transformaciones de texto en un solo paso\n",
        "df_spark = df_spark.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "df_spark = df_spark.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "df_spark = df_spark.withColumn('product_name_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'product_name_tokens')))\n",
        "df_spark = df_spark.withColumn('primary_category_cleaned', lemmatize_text_spark_udf(F.concat_ws(' ', 'primary_category_tokens')))\n",
        "\n",
        "# TF-IDF en 'product_name' y 'primary_category'\n",
        "# Asumiendo que 'loaded_model' y 'model_category' contienen los modelos que necesitas\n",
        "nuevos_datos_spark = loaded_model.transform(nuevos_datos_spark)\n",
        "nuevos_datos_spark = model_category.transform(nuevos_datos_spark)\n",
        "\n",
        "# Reducción de palabras en las columnas lematizadas\n",
        "nuevos_datos_spark = nuevos_datos_spark.withColumn('product_name_cleaned', F.expr(\"substring_index(product_name_cleaned, ' ', 2)\"))\n",
        "nuevos_datos_spark = nuevos_datos_spark.withColumn('primary_category_cleaned', F.expr(\"substring_index(primary_category_cleaned, ' ', 2)\"))\n",
        "\n",
        "# Asumiendo que las columnas de características son 'name_tfidf', 'category_tfidf', 'product_name_cleaned', y 'primary_category_cleaned'\n",
        "feature_columns = ['name_tfidf', 'category_tfidf', 'product_name_cleaned', 'primary_category_cleaned']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "\n",
        "# Ajustar columnas de características\n",
        "nuevos_datos_spark = vector_assembler.transform(nuevos_datos_spark)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones_nuevos_datos = loaded_model.transform(nuevos_datos_spark)\n",
        "\n",
        "# Visualizar las predicciones\n",
        "predicciones_nuevos_datos.select('prediction').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDTDpRgmS_cI"
      },
      "source": [
        "# de aca abajo se mantiene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "E6f5dNUhIfzG",
        "outputId": "37f00f37-4334-4c2f-c745-def4f30afd38"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenize_and_clean_spark_udf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a204b17f14db>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tokenización y limpieza en 'product_name' y 'primary_category'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnuevos_datos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnuevos_datos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'product_name_tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_and_clean_spark_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'product_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnuevos_datos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnuevos_datos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'primary_category_tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_and_clean_spark_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'primary_category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenize_and_clean_spark_udf' is not defined"
          ]
        }
      ],
      "source": [
        "# Crear un DataFrame de Spark con el nuevo registro\n",
        "nuevos_datos = spark.createDataFrame([nuevo_registro], schema=schema)\n",
        "\n",
        "# Tokenización y limpieza en 'product_name' y 'primary_category'\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "\n",
        "# Lematización en 'product_name' y 'primary_category'\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_cleaned', lemmatize_text_spark_udf('product_name_tokens'))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_cleaned', lemmatize_text_spark_udf('primary_category_tokens'))\n",
        "\n",
        "# TF-IDF en 'product_name' y 'primary_category'\n",
        "nuevos_datos = model_name.transform(nuevos_datos)\n",
        "nuevos_datos = model_category.transform(nuevos_datos)\n",
        "\n",
        "# Reducción de palabras en las columnas lematizadas\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_cleaned', F.expr(\"substring_index(product_name_cleaned, ' ', 2)\"))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_cleaned', F.expr(\"substring_index(primary_category_cleaned, ' ', 2)\"))\n",
        "\n",
        "# Ajustar columnas de características\n",
        "nuevos_datos_assembled = vector_assembler.transform(nuevos_datos).select('features')\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones_nuevos_datos = model.transform(nuevos_datos_assembled)\n",
        "\n",
        "# Visualizar las predicciones\n",
        "predicciones_nuevos_datos.select('prediction').show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY6kwsIByOX3"
      },
      "outputs": [],
      "source": [
        "# Crear un DataFrame de Spark con el nuevo registro\n",
        "nuevos_datos = spark.createDataFrame([nuevo_registro], schema=schema)\n",
        "\n",
        "# Tokenización y limpieza en 'product_name' y 'primary_category'\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_tokens', tokenize_and_clean_spark_udf('product_name'))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_tokens', tokenize_and_clean_spark_udf('primary_category'))\n",
        "\n",
        "# Lematización en 'product_name' y 'primary_category'\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_cleaned', lemmatize_text_spark_udf('product_name_tokens'))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_cleaned', lemmatize_text_spark_udf('primary_category_tokens'))\n",
        "\n",
        "# TF-IDF en 'product_name' y 'primary_category'\n",
        "nuevos_datos = model_name.transform(nuevos_datos)\n",
        "nuevos_datos = model_category.transform(nuevos_datos)\n",
        "\n",
        "# Reducción de palabras en las columnas lematizadas\n",
        "nuevos_datos = nuevos_datos.withColumn('product_name_cleaned', F.expr(\"substring_index(product_name_cleaned, ' ', 2)\"))\n",
        "nuevos_datos = nuevos_datos.withColumn('primary_category_cleaned', F.expr(\"substring_index(primary_category_cleaned, ' ', 2)\"))\n",
        "\n",
        "# Ajustar columnas de características\n",
        "nuevos_datos_assembled = vector_assembler.transform(nuevos_datos).select('features')\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones_nuevos_datos = model.transform(nuevos_datos_assembled)\n",
        "\n",
        "# Crear un DataFrame de mapeo entre prediction y Size\n",
        "size_label_mapping = [\n",
        "    (0.0, 'XS2'),\n",
        "    (1.0, 'XS3'),\n",
        "    (2.0, 'XS'),\n",
        "    (3.0, 'M'),\n",
        "    (4.0, 'S'),\n",
        "    (5.0, 'L'),\n",
        "    (6.0, 'EO'),\n",
        "    (7.0, 'XL'),\n",
        "    (8.0, 'LO'),\n",
        "    (9.0, 'O'),\n",
        "    (10.0, 'XXL')\n",
        "]\n",
        "\n",
        "size_label_mapping_df = spark.createDataFrame(size_label_mapping, ['label', 'Size'])\n",
        "\n",
        "# Realizar join con el DataFrame de mapping\n",
        "predicciones_nuevos_datos_with_size = predicciones_nuevos_datos.join(\n",
        "    size_label_mapping_df,\n",
        "    predicciones_nuevos_datos.prediction == size_label_mapping_df.label\n",
        ")\n",
        "\n",
        "# Seleccionar solo las columnas necesarias\n",
        "result_df = predicciones_nuevos_datos_with_size.select('prediction', 'Size')\n",
        "\n",
        "# Visualizar las predicciones con el tamaño correspondiente\n",
        "result_df.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}